#include <iostream>
#include <fstream>
#include <tuple>
#include <list>
#include <queue>
#include <vector>
#include <iterator>
#include <algorithm>
#include <string>
#include <random>
#include <limits>
#include <unordered_set>
#include <unordered_map>
using namespace std;

// g++ -Wl,--no-as-needed -g -Wall -std=c++11 -o rf rf.cc -lpthread

// No better solution for the type-obsessed languages but to create a type
//  that can play for the both sides
struct Variable {
    enum Type { Unknown=0, Categorical=1, Continuous=2 }; // Categorical is always considered unordered below
    Type type;
    union {
        long long asIntegral;
        double    asFloating;
    };
    friend ostream& operator<< (ostream&, const Variable&);
    ostream& operator<< (ostream& out) const {
        switch(type){
            case Categorical: out << "(int)"   << asIntegral; break;
            case Continuous:  out << "(float)" << asFloating; break;
            default : out << "Unkown"; break;
        }
        return out;
    }
    Variable(void){ type = Unknown; asIntegral = 0; }
    explicit Variable(long long integral){ type = Categorical; asIntegral = integral; }
    explicit Variable(double    floating){ type = Continuous;  asFloating = floating; }
};

ostream& operator<< (ostream& out, const Variable& var) { return var.operator<<(out); }

// The DataRow abstraction is meant to be an interface between templated and
//  non-templated worlds. Although the whole RandomForests framework could
//  have been made templated, this brings an unnecessary generalization and
//  blows the code out of proportion, while in fact we only deal with two types
//  of variables: categorical (integral) and non-categorical (floating point)
class DataRow {
private:
    // all the elements of the row are stored in this vector
    vector<Variable> data;

    // helper: store element with index IDX of tuple with MAX elements
    template <int IDX, int MAX, typename... Args>
    struct STORE_TUPLE {
        static void store(vector<Variable>& d, const std::tuple<Args...>& t) {
            auto element = std::get<IDX>(t);
            if( std::is_integral<decltype(element)>::value )
                d.push_back( Variable((long long) element ) );
            else
                d.push_back( Variable((double   ) element) );
            STORE_TUPLE<IDX+1,MAX,Args...>::store(d,t);
        }
    };

    // partial specialization to end the recursion
    template <int MAX, typename... Args>
    struct STORE_TUPLE<MAX,MAX,Args...> {
        static void store(vector<Variable>& d, const std::tuple<Args...>& t) {}
    };

    friend class DataFrame;

public:

    // interfacing DataRow with tuple
    template <typename... Args>
    DataRow& operator=(const tuple<Args...> &t) noexcept {
        data.resize(0);
        STORE_TUPLE<0,sizeof...(Args),Args...>::store(data,t);
        return *this;
    }

    template <typename... Args>
    DataRow(const tuple<Args...> &t) noexcept {
        STORE_TUPLE<0,sizeof...(Args),Args...>::store(data,t);
    }

    // subsetting
          Variable& operator[](unsigned int i)       { return data[i]; }
    const Variable& operator[](unsigned int i) const { return data[i]; }

    ostream& operator<< (ostream& out) const {
        copy(data.cbegin(),data.cend(),ostream_iterator<Variable>(out," "));
        return out;
    }

    DataRow(void){}
    // copy, and move c-tors will be generated by the compiler
};

ostream& operator<< (ostream& out, const DataRow& dr) { return dr.operator<<(out); }

// abstraction for grouping DataRows together
class DataFrame {
private:
    vector<unsigned int> schema; // 1 - continuous, >=2 - number of levels in categorical
    vector<DataRow> rows;

public:
    const vector<unsigned int>& getSchema(void) const { return schema; }
    unsigned int nrow(void) const { return rows.size(); }
    unsigned int ncol(void) const { return schema.size(); }

    template<typename T>
    bool cbind(const vector<T> &col, unsigned int nLevels=1) {
        // check if number of rows matchs number of elements in column
        if( col.size() != rows.size() && rows.size() != 0 )
            return false;
        // in case the DataFrame is empty initialize it with the column
        if( rows.size() == 0 ) rows.resize( col.size() );
        // the two options: categorical/integral and floating/continuous
        if( std::is_integral<T>::value ){
            for(unsigned i=0; i<col.size(); ++i)
                rows[i].data.emplace_back((long long)col[i]);
            // deduce number of levels automatically
            unordered_set<long long> unique;
            copy(col.cbegin(), col.cend(), inserter(unique,unique.begin()));
            // store number of found or provided levels
            if( nLevels > unique.size() )
                schema.push_back( nLevels );
            else
                schema.push_back( unique.size() );
        } else {
            for(unsigned i=0; i<col.size(); ++i)
                rows[i].data.emplace_back((double)col[i]);
            // mark the column as continuous
            schema.push_back(1);
        }
        return true;
    }

    bool rbind(DataRow && row) {
        // check if number of elements in the row agrees with the expectation
        if( row.data.size() != schema.size() && schema.size() > 0 )
            return false;
        // check if we start fresh
        if( schema.size() == 0 ){
            // initialize the empty DataFrame with the row
            rows.push_back(row);
            transform(row.data.cbegin(), row.data.cend(), back_inserter(schema),
                [](const Variable& var){ return (var.type == Variable::Categorical ? 2 : 1) ; }
            );
        } else {
            // make sure we preserve the schema but do nothing about number of levels assuming it includes the binded
            if( !equal(row.data.cbegin(), row.data.cend(), schema.cbegin(),
                     [](const Variable& var, int type){
                         return (var.type == Variable::Categorical && type >= 2) ||
                                (var.type == Variable::Continuous  && type == 1) ;
                     }
                 )
            ) return false;
        }
        rows.push_back( move(row) );
        return true;
    }

          DataRow& operator[](unsigned int i)       { return rows[i]; }
    const DataRow& operator[](unsigned int i) const { return rows[i]; }

    ostream& print(ostream& out, int nrows=-1) const {
        copy( rows.cbegin(),
              (nrows<0 ? rows.cend() : rows.cbegin()+nrows),
              ostream_iterator<DataRow>(out,"\n")
        );
        return out;
    }
};

ostream& operator<<(ostream& out, const DataFrame& df){ return df.print(out); }

class Tree {
private:
    struct Node {
        Variable value;
        int position;
        int left_child, right_child;
        Node(void) : value(),  position(0), left_child(0), right_child(0){}
        explicit Node(double v) : value((double)    v), position(0), left_child(0), right_child(0){}
        explicit Node(long   v) : value((long long) v), position(0), left_child(0), right_child(0){}
    };

    friend class RandomForest;

    Tree *parent, *left_subtree, *right_subtree; // temporary, used before we pack nodes in a vector below
    size_t tree_size;
    vector<Node> nodes; // vectorized tree - canonical representation after I'm done with growing it

    // for prunning
    double rss, sum, sum2;
    size_t set_size;

    // pack pointers into a vector, free dynamically allocated memory
    size_t vectorize(vector<Node>& dest) {
        // sanity checks
        //  uninitialized?
        if( nodes.size() == 0 ) return 0;
        //  broken invariant (either vectorized or both subtrees present)?
        if( ( left_subtree == 0 && right_subtree != 0 ) ||
            ( left_subtree != 0 && right_subtree == 0 ) ) return 0;
        // already vectorized?
        if( tree_size == 0 ) return 0;

        // pre-order traversal
        dest.push_back(nodes[0]);
        Node& local_root = dest.back();

        size_t size = 1;

        // recure if not a terminal node
        if( left_subtree != 0 && right_subtree != 0 ){
            local_root.left_child  = dest.size();
            size += left_subtree->vectorize(dest);
            delete left_subtree;
            left_subtree = 0;

            local_root.right_child = dest.size();
            size += right_subtree->vectorize(dest);
            delete right_subtree;
            right_subtree = 0;
        }

        // no longer need this variable -> use it to indicate that tree is vectorized
        tree_size = 0;

        return size;
    }

public:
    Variable traverse(const DataRow& row, const Node& root) const {
        // is it a leaf/terminal_node?
        if( root.left_child == 0 && root.right_child == 0 )
            return root.value;

        if( root.value.type == Variable::Continuous ){
            if( root.value.asFloating > row[root.position].asFloating )
                return traverse(row,nodes[root.left_child]);
            else
                return traverse(row,nodes[root.right_child]);
        }
        if( root.value.type == Variable::Categorical ){
            // only binary-level categories are managed
            if( root.value.asIntegral == row[root.position].asIntegral )
                return traverse(row,nodes[root.left_child]);
            else
                return traverse(row,nodes[root.right_child]);
        }
        // the root is neither Continuous nor Categorical -> error
        return Variable();
    }
    Variable predict(const DataRow& row) const {
        // is tree initialized? if not return default Variable as a sign of error
        if( nodes.size() == 0 ) return Variable(); 
        // is root node initialized?
        if( nodes[0].value.type == Variable::Unknown ) return Variable(); 
        // all looks good
        return traverse(row,nodes[0]);
    }

    bool load(istream &input){
        return true;
    }
    bool save(ostream &output){
        if( left_subtree != 0 || right_subtree != 0 )
            return false;
        for(unsigned int n=0; n<nodes.size(); n++)
            output << n
                   << "," << nodes[n].value
                   << "," << nodes[n].position
                   << "," << nodes[n].left_child
                   << "," << nodes[n].right_child
                   << endl;
        return true;
    }

    Tree(void) : parent(0), left_subtree(0), right_subtree(0), tree_size(0), nodes(0), rss(0) {}
    // tree is an owner of its subtrees
    ~Tree(void){
        if( left_subtree  ) delete left_subtree;
        if( right_subtree ) delete right_subtree;
    }
};

class RandomForest {
private:
public:
    typedef list<pair<unsigned int,unsigned int>> SplitVars; // variable index and level (if categorical)

    std::default_random_engine rState;

    SplitVars generateRandomSplitVars(const vector<unsigned int> &schema, const vector<unsigned int>& predictorsIdx, unsigned int mtry){
        SplitVars vars;
        default_random_engine dre(rState);
        uniform_int_distribution<unsigned int> uid(0, predictorsIdx.size()-1), uid_l;
        generate_n( back_inserter(vars),
                    mtry,
                    [&uid,&uid_l,&dre,&schema,&predictorsIdx](void){
                        unsigned int idx = predictorsIdx[ uid(dre) ];
                        unsigned int level = (schema[idx]>1 ? uid_l(dre)%schema[idx] : 0);
                        return pair<unsigned int,unsigned int>(idx,level);
                    }
        );
        return vars;
    }

    vector<unsigned int> sample(unsigned int nTotal, unsigned int nSampled, bool replace = false){
        // definitely, there is room for improvement below
        vector<unsigned int> retval(nTotal);
        if( !replace ){
            iota(retval.begin(),retval.end(),0);
            shuffle(retval.begin(),retval.end(),rState);
        } else {
            default_random_engine dre(rState);
            uniform_int_distribution<> uid(0, nTotal);
            generate_n( retval.begin(), (nSampled<nTotal?nSampled:nTotal), [&uid,&dre](void){ return uid(dre); } );
        }
        return vector<unsigned int>(retval.begin(), retval.begin() + (nSampled<nTotal?nSampled:nTotal));
    }

    // implementation of CART with gini/entrophy/rms purity metrics
    Tree* findBestSplits(const DataFrame& df,
                         unsigned int responseIdx,
                         const SplitVars& vars,
                         const vector<unsigned int>& subset = {}
    ){
        // safety: nothing to split on? 
        if( vars.empty() ) return new Tree();

#define MIN_ENTRIES 5

        size_t size = subset.size();
        double sum = 0, sum2 = 0;
        for(unsigned int i=0; i<size; ++i){
            unsigned int row = subset[i];
            sum  += df[row][responseIdx].asFloating;
            sum2 += df[row][responseIdx].asFloating * df[row][responseIdx].asFloating;
        }
        double rss = sum2 - sum*sum/size;

        // do not grow tree beyond MIN_ENTRIES or less 
        if( size <= MIN_ENTRIES ){
            Tree::Node leaf( sum/size );
            Tree *retval = new Tree();
            retval->nodes.push_back(leaf);
            retval->tree_size = 1;
            retval->rss  = rss;
            retval->sum  = sum;
            retval->sum2 = sum2;
            retval->set_size = size;
            return retval;
        }

        // finding best split in regression is solving Eq 9.13 on p.307 of ESLII
        size_t bestSplitVar = 0;
        double bestSplitPoint = 0;
        double bestSplitImpurityDecrease = numeric_limits<double>::max();
        for(pair<unsigned int,unsigned int> var : vars){
            vector<unsigned int> indices(size);
            iota(indices.begin(),indices.end(),0);
            sort(indices.begin(),
                 indices.end(),
                 [&subset, &df, &var] (unsigned int i, unsigned int j) {
                     return df[ subset[i] ][var.first].asFloating < df[ subset[j] ][var.first].asFloating;
                 }
            );
            // functional form of Eq 9.13 on p.307 of ESLII
            std::function<double(double,double,size_t,double,double,size_t)> metric =
                [](double sum_l, double sum2_l, size_t size_l, double sum_r, double sum2_r, size_t size_r){
                    return (size_l ? (sum2_l - sum_l * sum_l / size_l) : 0) +
                           (size_r ? (sum2_r - sum_r * sum_r / size_r) : 0);
                };
            // start with all points being on one side of the split
            double sum_r = sum, sum2_r = sum2, sum_l = 0, sum2_l = 0;
            size_t bestSplitPointSoFar = 0, size_l = 0, size_r = size;
            double bestMetricSoFar = metric(sum_l,sum2_l,size_l,sum_r,sum2_r,size_r);
            // run over the sorted df representation
            for(unsigned int index : indices){
                unsigned int row = subset[index];
                // advancing the split - moving a point from right to left of the split
                sum_r  -= df[row][responseIdx].asFloating;
                sum2_r -= df[row][responseIdx].asFloating * df[row][responseIdx].asFloating;
                sum_l  += df[row][responseIdx].asFloating;
                sum2_l += df[row][responseIdx].asFloating * df[row][responseIdx].asFloating;
                size_r--;
                size_l++;
                double newMetric = metric(sum_l,sum2_l,size_l,sum_r,sum2_r,size_r);
                if( newMetric < bestMetricSoFar ){
                    bestMetricSoFar     = newMetric;
                    bestSplitPointSoFar = row;
                }
            }
            if( bestMetricSoFar < bestSplitImpurityDecrease ){
                bestSplitVar   = var.first;
                bestSplitPoint = df[bestSplitPointSoFar][var.first].asFloating;
                bestSplitImpurityDecrease = bestMetricSoFar;
            }
        }

//    cout << "bestSplitVar: " << bestSplitVar << " bestSplitPoint: " << bestSplitPoint << " bestSplitImpurityDecrease: " << bestSplitImpurityDecrease << " sample:" << subset.size() << endl; 

            vector<unsigned int> left_subset, right_subset;
            for(unsigned int i : subset)
                switch(  df[i][bestSplitVar].type ){
                    case Variable::Continuous :
                        if( df[i][bestSplitVar].asFloating < bestSplitPoint )
                            left_subset.push_back(i);
                        else
                            right_subset.push_back(i);
                    break ;
//                    case Variable::Categorical:
//                        if( df[i][bestSplitVar].asIntegral == bestCut->first.second )
//                            left_subset.push_back(i);
//                        else
//                            right_subset.push_back(i);
 //                   break ;
                    default : return new Tree(); break;
                }

        Tree *tree = new Tree();
        tree->rss  = rss;
        tree->sum  = sum;
        tree->sum2 = sum2;
        tree->set_size = size;

        // Continue growing tree until any of the subsets are smaller the MIN_ENTRIES
        if( left_subset.size() > MIN_ENTRIES && right_subset.size() > MIN_ENTRIES ){

            // good place to use the threads
            Tree *left_subtree  = findBestSplits(df, responseIdx, vars, left_subset);
            Tree *right_subtree = findBestSplits(df, responseIdx, vars, right_subset);

            left_subtree->parent  = tree;
            right_subtree->parent = tree;
            tree->left_subtree  = left_subtree;
            tree->right_subtree = right_subtree;
            tree->nodes.resize(1);
            tree->tree_size = 1 + left_subtree->tree_size + right_subtree->tree_size;

            // copy the local root node
            Tree::Node& local_root = tree->nodes[0];
            local_root.position = bestSplitVar;
            if( df.getSchema()[bestSplitVar] == 1 ){
                local_root.value.type = Variable::Continuous;
                local_root.value.asFloating = bestSplitPoint;
            } else {
                local_root.value.type = Variable::Categorical;
                local_root.value.asIntegral = 0; // to be implemented
            }

        } else {
            Tree::Node leaf( sum/size );
            tree->tree_size = 1;
            tree->nodes.push_back(leaf);
        }
#undef MIN_ENTRIES

        return tree;
    }

    // weakest link pruning as prescribed in ESLII p.308
    void prune(Tree *tree, double alpha){
        vector<Tree*> candsForCollapse;
        double rssTotal = 0;

        // traverse the tree with local FIFO simulating stack of recursion
        queue<Tree*> fifo;
        fifo.push(tree);
        while( !fifo.empty() ){
            Tree *t = fifo.front();
            fifo.pop();
            Tree *t_l = t->left_subtree;
            Tree *t_r = t->right_subtree;
            if( t_l && t_r ){
                // look ahead for leafs
                if( t_l->left_subtree == 0 && t_l->right_subtree == 0 &&
                    t_r->left_subtree == 0 && t_r->right_subtree == 0 ){
                    candsForCollapse.push_back(t);
                } else {
                    fifo.push(t_l);
                    fifo.push(t_r);
                }
            }
            if( t_l == 0 && t_r == 0 )
                rssTotal += t->rss;
        }

        function<bool(Tree*,Tree*)> rssGreaterEq = [](Tree* i, Tree* j){ return i->rss >= j->rss; };

        make_heap(candsForCollapse.begin(), candsForCollapse.end(),rssGreaterEq);

// check why candsForCollapse every become empty
        while( rssTotal < alpha * tree->tree_size && tree->tree_size > 1 && !candsForCollapse.empty() ){
            pop_heap(candsForCollapse.begin(), candsForCollapse.end(), rssGreaterEq);
            Tree *t = candsForCollapse.back();
            candsForCollapse.pop_back();
            // collapsing t: chop-off the leafs
            rssTotal -= t->left_subtree->rss;
            rssTotal -= t->right_subtree->rss;
            delete t->left_subtree;
            t->left_subtree = 0;
            delete t->right_subtree;
            t->right_subtree = 0;
            // leaf value has to become average rather than split point
            t->nodes[0].value.asFloating = t->sum/t->set_size;
            rssTotal += t->rss;
            // parent may become a candidate for next collapse
            Tree *p = t->parent;
            if( p->tree_size == 2 ){
                p->tree_size = 1;
                candsForCollapse.push_back(p);
                push_heap(candsForCollapse.begin(), candsForCollapse.end(), rssGreaterEq);
            } else
                p->tree_size--;
        }

    }

    vector<Tree> ensemble;

public:
    double regress(const DataRow& row) const {
        double sum = 0.;
        for(const auto &tree : ensemble)
            sum += tree.predict(row).asFloating;
        return sum/ensemble.size();
    }

    int   classify(const DataRow& row) const { return 0; }

    void train(const DataFrame& df, const vector<unsigned int>& predictorsIdx, unsigned int responseIdx) {
        if( df.nrow() < 1 ) return ;
        rState.seed(0);
        const int nTrees = 1;
        for(unsigned int t=0; t<nTrees; t++){
            SplitVars vars( generateRandomSplitVars( df.getSchema(), predictorsIdx, floor(predictorsIdx.size()>15?predictorsIdx.size()/3:5) ) );//(unsigned int)sqrt(predictorsIdx.size()) ) );
//for(auto s : vars) cout << "s.first = "<<s.first << " s.second = "<< s.second << endl;
//            future<Tree> ft = async(std::launch::async, pickStrongestCuts, df, responseIdx, vars, sample(df.nrow(),df.nrow()*0.5));
            Tree *tree = findBestSplits(df, responseIdx, vars, sample(df.nrow(),df.nrow()*0.5));
cout<<tree->tree_size<<endl;
            prune(tree,10);
            vector<Tree::Node> nodes;
            nodes.reserve(tree->tree_size);
            tree->vectorize(nodes);
            tree->nodes.swap(nodes);
tree->save(cout);
            ensemble.push_back( move(*tree) );
        }
    }

    RandomForest(void){}

//    load/save
};

DataFrame oneHOTencode(const vector<int> &col, unordered_set<int> levels = {}, bool nMinusOne = true){
    DataFrame df;
    // if levels are not provided, deduce them; provided two - check
    if( levels.size() <= 2 ){
        levels.clear();
        copy(col.cbegin(), col.cend(), inserter(levels,levels.begin()));
    }
    // do nothing for binary levels
    if( levels.size() == 2 ){
        df.cbind(col);
        return df;
    }
    // encode
    for(auto l: levels){
        // N-1 levels? - make 0th level - all zeros
        if( nMinusOne && l == *levels.begin() ) continue;
        // turn each level into a binary match/mismatch column
        vector<char> binary( col.size() );
        transform(col.cbegin(), col.cend(), binary.begin(), [&l] (int i){ return i==l; } );
        df.cbind(binary);
    }
    return df;
}

template<int IDX, int NMAX, typename... Args>
struct READ_TUPLE {
    static bool read(istream &in, tuple<Args...> &t){
        if( in.eof() ) return false;
        in >> get<IDX>(t);
        return READ_TUPLE<IDX+1,NMAX,Args...>::read(in,t);
    }
};
template<int NMAX, typename... Args>
struct READ_TUPLE<NMAX,NMAX,Args...>{
    static bool read(istream &in, tuple<Args...> &t){ return true; }
};
template <typename... Args>

bool read_tuple(istream &in, tuple<Args...> &t) noexcept {
    return READ_TUPLE<0,sizeof...(Args),Args...>::read(in,t);
}

void setSeparators(istream& input){
    struct field_reader: std::ctype<char> {
        field_reader(): std::ctype<char>(get_table()) {}

        static std::ctype_base::mask const* get_table() {
            static std::vector<std::ctype_base::mask> 
                rc(table_size, std::ctype_base::mask());

            rc['\n'] = std::ctype_base::space;
            rc[':']  = std::ctype_base::space;
            rc[',']  = std::ctype_base::space;
            return &rc[0];
        }
    };
    input.imbue(std::locale(std::locale(), new field_reader()));
}

void readHeader(istream& input, unsigned int ncol){
    unordered_map<string,unsigned int> dict;
    class my_dict_output_iterator : public iterator<output_iterator_tag,typename unordered_map<string,unsigned int>::value_type> {
        private:
            unsigned int counter;
        protected:
            unordered_map<string,unsigned int>& container;
        public:
            explicit my_dict_output_iterator(unordered_map<string,unsigned int> &c) : counter(0), container(c){ }
            my_dict_output_iterator operator= (const string& str){
                container.insert( make_pair(str,counter++) );
                return *this;
            }
            my_dict_output_iterator& operator*  (void){ return *this; }
            my_dict_output_iterator& operator++ (void){ return *this; }
            my_dict_output_iterator& operator++ (int) { return *this; }
    };
    copy_n(istream_iterator<string>(input), ncol, my_dict_output_iterator(dict));
}

DataFrame read1(void){
    // require(MASS)
    // xy <- mvrnorm( 1000000, c(1,2), matrix(c(3,2,2,4),ncol=2) )
    // plot(xy[sample(nrow(xy),10000),], xlab="x", ylab="y", pch=1)
    // write.csv(file="one.csv",x=xy[sample(nrow(xy),10000),])
    ifstream input("one.csv");
    setSeparators(input);
    readHeader(input,3);
    typedef tuple<string,float,float> Format;
    DataFrame df;
    Format tmp;
    for(unsigned int row=0; read_tuple(input,tmp); row++){
        tuple<float,float> r12 = make_tuple(
                get<1>(tmp), get<2>(tmp)
        );
        df.rbind( DataRow(r12) );
    }
    return df;
}

DataFrame read2(void){
    ifstream input("../trigger/pt/SingleMu_Pt1To1000_FlatRandomOneOverPt.csv");
    setSeparators(input);
    readHeader(input,53);
    typedef tuple<int,float,float,float,int,float,float,float,float,float,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,
                  int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int> Format;
#define dPhi12_0 13
#define dPhi12_1 14
#define dPhi23_0 19
#define dPhi23_1 20
#define dPhi34_0 23
#define dPhi34_1 24
#define dPhi13_0 15
#define dPhi13_1 16
#define dPhi14_0 17
#define dPhi14_1 18
#define dPhi24_0 21
#define dPhi24_1 22
#define muPtGen  1
    DataFrame df;
    Format tmp;
    for(unsigned int row=0; read_tuple(input,tmp); row++){
        if( get<11>(tmp) == 15 ){
            tuple<float,float,float,float,float,float,float> dPhis = make_tuple(
                abs(get<dPhi12_0>(tmp)), abs(get<dPhi23_0>(tmp)), abs(get<dPhi34_0>(tmp)),
                abs(get<dPhi13_0>(tmp)), abs(get<dPhi14_0>(tmp)), abs(get<dPhi24_0>(tmp)),
                1./get<muPtGen>(tmp)
            );
            df.rbind( DataRow(dPhis) );
        }
        if( get<12>(tmp) == 15 ){
            tuple<float,float,float,float,float,float,float> dPhis = make_tuple(
                abs(get<dPhi12_1>(tmp)), abs(get<dPhi23_1>(tmp)), abs(get<dPhi34_1>(tmp)),
                abs(get<dPhi13_1>(tmp)), abs(get<dPhi14_1>(tmp)), abs(get<dPhi24_1>(tmp)),
                1./get<muPtGen>(tmp)
            );
            df.rbind( DataRow(dPhis) );
        }
    }
    return df;
}


int main(void){
    RandomForest rf;

    DataFrame df( read2() );
    vector<unsigned int> predictorsIdx = {0,1,2,3,4,5};
    rf.train(df,predictorsIdx,6);

//    DataFrame df( read1() );
//    vector<unsigned int> predictorsIdx = {0};
//    rf.train(df,predictorsIdx,1);

//    rf.ensemble[0].save(cout);

    double bias = 0, var = 0;
    long cnt = 0;
    for(unsigned int row = 0; row>=0 && row < df.nrow(); row++,cnt++){
        double prediction = rf.regress( df[row] );
        double truth      = df[row][6].asFloating; // 6
// cout << df[row] <<endl;
//        cout << "prediction = "<<prediction <<" truth= "<<truth<<endl;
//        double prediction = 1./rf.regress( df[row] );
//        double truth      = 1./df[row][6].asFloating;
        bias +=  prediction - truth;
        var  += (prediction - truth) * (prediction - truth);
    }
    double sd = sqrt((var - bias*bias/cnt)/(cnt - 1));
    bias /= cnt;
    cout << "bias = "<< bias << " sd = " << sd << endl;

    return 0;
}
